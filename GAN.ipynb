{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 5, 2, activation=tf.nn.leaky_relu)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, 2, activation=tf.nn.leaky_relu)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 5, 2, activation=tf.nn.leaky_relu)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.3)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(256, 5, 2, activation=tf.nn.leaky_relu)\n",
    "        self.dropout4 = tf.keras.layers.Dropout(0.3)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dropout1(self.conv1(x))\n",
    "        x = self.dropout2(self.conv2(x))\n",
    "        x = self.dropout3(self.conv3(x))\n",
    "        x = self.dropout4(self.conv4(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def _reshape_func(x):\n",
    "            dims = x.get_shape().as_list()\n",
    "            return tf.reshape(x, [dims[0], 8, 8, 256])\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(8 * 8 * 256)\n",
    "        self.reshape = _reshape_func\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(256, 5, 2, activation=tf.nn.relu, padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(128, 5, 2, activation=tf.nn.relu, padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(64, 5, 2, activation=tf.nn.relu, padding='same')\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(32, 5, 2, activation=tf.nn.relu, padding='same')\n",
    "        self.conv5 = tf.keras.layers.Conv2DTranspose(16, 5, 2, activation=tf.nn.relu, padding='same')\n",
    "        self.conv6 = tf.keras.layers.Conv2DTranspose(3, 3, 1, activation=tf.nn.tanh, padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu = True\n",
    "# if gpu:\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#     print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# else:\n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser()\n",
    "args.add_argument('--learning_rate', type = float,\n",
    "     default = 5e-4, help = 'initial learning rate')\n",
    "args.add_argument('--gp_lambda', type = float,\n",
    "     default = 20, help = 'lambda of gradient penalty')\n",
    "args.add_argument('--n_epoch', type = int,\n",
    "    default = 40000, help = 'max # of epoch')\n",
    "args.add_argument('--n_update_dis', type = int,\n",
    "    default = 5, help = '# of updates of discriminator per update of generator')\n",
    "args.add_argument('--noise_dim', type = int,\n",
    "    default = 128, help = 'dimension of random noise')\n",
    "args.add_argument('--batch_size', type = int,\n",
    "    default = 32, help = '# of batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'learning_rate': 5e-4,\n",
    "    'gp_lambda': 20,\n",
    "    'n_epoch': 40000,\n",
    "    'n_update_dis': 5,\n",
    "    'noise_dim': 128,\n",
    "    'batch_size': 32\n",
    "    }\n",
    "class config:\n",
    "    save_tanker_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\TA\\\\\"\n",
    "    save_container_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\CS\\\\\"\n",
    "    save_bulkcarrier_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\BC\\\\\"\n",
    "    save_generalcargo_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\GC\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplefile(file_li):\n",
    "    l = list(range(len(file_li)))\n",
    "    kp_idx = random.sample(l, 50)\n",
    "    kp_idx.sort()\n",
    "    file_li_ = [file_li[idx] for idx in kp_idx]\n",
    "    return file_li_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def channel4to3(img):\n",
    "#     if len(img.shape) > 2 and img.shape[2] == 4:\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "#     return img\n",
    "def channel4to3(img):\n",
    "    if img is not None and len(img.shape) > 2 and img.shape[2] == 4:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataset(args):\n",
    "    # tanker class\n",
    "    \n",
    "    tanker_dir = os.listdir(config.save_tanker_target_path)\n",
    "    tanker_dir = samplefile(tanker_dir)\n",
    "    all_digits_tanker = []\n",
    "    all_labels_tanker = []\n",
    "    \n",
    "#     for item in tanker_dir:\n",
    "#         file_path = config.save_tanker_target_path + item\n",
    "#         print(\"Loading:\", file_path)\n",
    "#         img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "#         img = channel4to3(img)\n",
    "#         if img is not None:\n",
    "#             img = channel4to3(img)\n",
    "#             all_digits_tanker.append(img)\n",
    "#             all_labels_tanker.append(0)\n",
    "#         else:\n",
    "#             print(f\"Failed to load image: {file_path}\")\n",
    "    for item in tanker_dir:\n",
    "        img = cv2.imread(config.save_tanker_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_tanker.append(img)\n",
    "        all_labels_tanker.append(0)\n",
    "    all_digits_tanker = np.array(all_digits_tanker)\n",
    "    all_labels_tanker = np.array(all_labels_tanker)\n",
    "\n",
    "    # container class\n",
    "    container_dir = os.listdir(config.save_container_target_path)\n",
    "    container_dir = samplefile(container_dir)\n",
    "    all_digits_container = []\n",
    "    all_labels_container = []\n",
    "    for item in container_dir:\n",
    "        img = cv2.imread(config.save_container_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_container.append(img)\n",
    "        all_labels_container.append(1)\n",
    "    all_digits_container = np.array(all_digits_container)\n",
    "    all_labels_container = np.array(all_labels_container)\n",
    "\n",
    "    # bulkcarrier class\n",
    "    bulkcarrier_dir = os.listdir(config.save_bulkcarrier_target_path)\n",
    "    bulkcarrier_dir = samplefile(bulkcarrier_dir)\n",
    "    all_digits_bulkcarrier = []\n",
    "    all_labels_bulkcarrier = []\n",
    "    for item in bulkcarrier_dir:\n",
    "        img = cv2.imread(config.save_bulkcarrier_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_bulkcarrier.append(img)\n",
    "        all_labels_bulkcarrier.append(2)\n",
    "    all_digits_bulkcarrier = np.array(all_digits_bulkcarrier)\n",
    "    all_labels_bulkcarrier = np.array(all_labels_bulkcarrier)\n",
    "\n",
    "    # general cargo class\n",
    "    generalcargo_dir = os.listdir(config.save_generalcargo_target_path)\n",
    "    generalcargo_dir = samplefile(generalcargo_dir)\n",
    "    all_digits_generalcargo = []\n",
    "    all_labels_generalcargo = []\n",
    "    for item in generalcargo_dir:\n",
    "        img = cv2.imread(config.save_generalcargo_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_generalcargo.append(img)\n",
    "        all_labels_generalcargo.append(3)\n",
    "    all_digits_generalcargo = np.array(all_digits_generalcargo)\n",
    "    all_labels_generalcargo = np.array(all_labels_generalcargo)\n",
    "\n",
    "    all_digits = np.concatenate([all_digits_tanker,all_digits_container,all_digits_bulkcarrier,all_digits_generalcargo],axis=0)\n",
    "    all_digits = (all_digits.astype(\"float32\")/ 255.0)*2-1\n",
    "\n",
    "    all_labels = np.concatenate([all_labels_tanker, all_labels_container,all_labels_bulkcarrier,all_labels_generalcargo], axis=0)\n",
    "    all_labels = keras.utils.to_categorical(all_labels, 4)\n",
    "    all_labels = all_labels*2-1\n",
    "    print('input shape: ',all_digits.shape)\n",
    "    print('label shape: ', all_labels.shape)\n",
    "\n",
    "    print(args)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_digits,all_labels)).shuffle(250).batch(args.batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover(image):\n",
    "    image_ = image.numpy()\n",
    "    image__ = np.round(0.5*(image_+1)*255).astype(np.int32)\n",
    "    return image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(fake_sample, epoch):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(fake_sample.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(fake_sample[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/' % epoch)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_gen(args, one_hot_labels):\n",
    "    batch_size = one_hot_labels.get_shape().as_list()[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        noise = tf.random.uniform([batch_size, args.noise_dim], -1.0, 1.0)\n",
    "        random_labels = tf.concat(\n",
    "            [noise, one_hot_labels], axis=1\n",
    "        )\n",
    "        fake_sample = args.gen(random_labels)\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[256 * 256]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, 256, 256, 4)\n",
    "        )\n",
    "\n",
    "        fake_sample = tf.concat([fake_sample, image_one_hot_labels], -1)\n",
    "\n",
    "        fake_score = args.dis(fake_sample)\n",
    "        loss = - tf.reduce_mean(fake_score)\n",
    "    gradients = tape.gradient(loss, args.gen.trainable_variables)\n",
    "    args.gen_opt.apply_gradients(zip(gradients, args.gen.trainable_variables))\n",
    "    args.gen_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_dis(args, real_sample, one_hot_labels):\n",
    "    batch_size = real_sample.get_shape().as_list()[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        noise = tf.random.uniform([batch_size, args.noise_dim], -1.0, 1.0)\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[256 * 256]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, 256, 256, 4)\n",
    "        )\n",
    "\n",
    "        noise_label = tf.concat(\n",
    "            [noise, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        fake_sample = args.gen(noise_label)\n",
    "        fake_sample = tf.concat([fake_sample, image_one_hot_labels], -1)\n",
    "        real_sample = tf.concat([real_sample, image_one_hot_labels], -1)\n",
    "\n",
    "        real_score = args.dis(real_sample)\n",
    "        fake_score = args.dis(fake_sample)\n",
    "\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        inter_sample = fake_sample * alpha + real_sample * (1 - alpha)\n",
    "        with tf.GradientTape() as tape_gp:\n",
    "            tape_gp.watch(inter_sample)\n",
    "            inter_score = args.dis(inter_sample)\n",
    "        gp_gradients = tape_gp.gradient(inter_score, inter_sample)\n",
    "        gp_gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradients), axis = [1, 2, 3]))\n",
    "        gp = tf.reduce_mean((gp_gradients_norm - 1.0) ** 2)\n",
    "\n",
    "        loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score) + gp * args.gp_lambda\n",
    "\n",
    "    gradients = tape.gradient(loss, args.dis.trainable_variables)\n",
    "    args.dis_opt.apply_gradients(zip(gradients, args.dis.trainable_variables))\n",
    "\n",
    "    args.dis_loss(loss)\n",
    "    args.adv_loss(loss - gp * args.gp_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(args, epoch):\n",
    "    noise = tf.random.uniform([64, args.noise_dim], -1.0, 1.0)\n",
    "    one_hot_labels = np.eye(4)[np.random.choice(4, 64)]\n",
    "    one_hot_labels = one_hot_labels * 2 - 1\n",
    "    random_labels = tf.concat(\n",
    "        [noise, one_hot_labels], axis=1\n",
    "    )\n",
    "    fake_sample = args.gen(random_labels)\n",
    "    fake_sample = recover(fake_sample)\n",
    "    plot(fake_sample, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    args.ds = Dataset(args)\n",
    "\n",
    "    # Initialize Networks\n",
    "    args.gen = Generator()\n",
    "    args.dis = Discriminator()\n",
    "\n",
    "    # Initialize Optimizer\n",
    "    args.gen_opt = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "    args.dis_opt = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    args.adv_loss = tf.keras.metrics.Mean(name = 'Adversarial_Loss')\n",
    "    args.gen_loss = tf.keras.metrics.Mean(name = 'Generator_Loss')\n",
    "    args.dis_loss = tf.keras.metrics.Mean(name = 'Discriminator_Loss')\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "input shape:  (200, 256, 256, 3)\n",
      "label shape:  (200, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gpu = True\n",
    "if gpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "else:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-4)\n",
    "    parser.add_argument('--gp_lambda', type=float, default=20)\n",
    "    parser.add_argument('--n_epoch', type=int, default=40000)\n",
    "    parser.add_argument('--n_update_dis', type=int, default=5)\n",
    "    parser.add_argument('--noise_dim', type=float,default = 128)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()  # Change this line\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "class config:\n",
    "    save_tanker_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\TA\\\\\"\n",
    "    save_container_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\CS\\\\\"\n",
    "    save_bulkcarrier_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\BC\\\\\"\n",
    "    save_generalcargo_target_path = \"C:\\\\Users\\\\THAHEER\\\\OneDrive\\\\Desktop\\\\mini\\\\DVTR\\\\DVTR\\\\UAV-view\\\\train\\\\GC\\\\\"\n",
    "\n",
    "\n",
    "def samplefile(file_li):\n",
    "    l = list(range(len(file_li)))\n",
    "    kp_idx = random.sample(l, 50)\n",
    "    kp_idx.sort()\n",
    "    file_li_ = [file_li[idx] for idx in kp_idx]\n",
    "    return file_li_\n",
    "\n",
    "def channel4to3(img):\n",
    "    if len(img.shape) > 2 and img.shape[2] == 4:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    return img\n",
    "\n",
    "def Dataset(args):\n",
    "    # tanker class\n",
    "    tanker_dir = os.listdir(config.save_tanker_target_path)\n",
    "    tanker_dir = samplefile(tanker_dir)\n",
    "    all_digits_tanker = []\n",
    "    all_labels_tanker = []\n",
    "    for item in tanker_dir:\n",
    "        img = cv2.imread(config.save_tanker_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_tanker.append(img)\n",
    "        all_labels_tanker.append(0)\n",
    "    all_digits_tanker = np.array(all_digits_tanker)\n",
    "    all_labels_tanker = np.array(all_labels_tanker)\n",
    "\n",
    "    # container class\n",
    "    container_dir = os.listdir(config.save_container_target_path)\n",
    "    container_dir = samplefile(container_dir)\n",
    "    all_digits_container = []\n",
    "    all_labels_container = []\n",
    "    for item in container_dir:\n",
    "        img = cv2.imread(config.save_container_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_container.append(img)\n",
    "        all_labels_container.append(1)\n",
    "    all_digits_container = np.array(all_digits_container)\n",
    "    all_labels_container = np.array(all_labels_container)\n",
    "\n",
    "    # bulkcarrier class\n",
    "    bulkcarrier_dir = os.listdir(config.save_bulkcarrier_target_path)\n",
    "    bulkcarrier_dir = samplefile(bulkcarrier_dir)\n",
    "    all_digits_bulkcarrier = []\n",
    "    all_labels_bulkcarrier = []\n",
    "    for item in bulkcarrier_dir:\n",
    "        img = cv2.imread(config.save_bulkcarrier_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_bulkcarrier.append(img)\n",
    "        all_labels_bulkcarrier.append(2)\n",
    "    all_digits_bulkcarrier = np.array(all_digits_bulkcarrier)\n",
    "    all_labels_bulkcarrier = np.array(all_labels_bulkcarrier)\n",
    "\n",
    "    # general cargo class\n",
    "    generalcargo_dir = os.listdir(config.save_generalcargo_target_path)\n",
    "    generalcargo_dir = samplefile(generalcargo_dir)\n",
    "    all_digits_generalcargo = []\n",
    "    all_labels_generalcargo = []\n",
    "    for item in generalcargo_dir:\n",
    "        img = cv2.imread(config.save_generalcargo_target_path + item, cv2.IMREAD_UNCHANGED)\n",
    "        img = channel4to3(img)\n",
    "        all_digits_generalcargo.append(img)\n",
    "        all_labels_generalcargo.append(3)\n",
    "    all_digits_generalcargo = np.array(all_digits_generalcargo)\n",
    "    all_labels_generalcargo = np.array(all_labels_generalcargo)\n",
    "\n",
    "    all_digits = np.concatenate([all_digits_tanker,all_digits_container,all_digits_bulkcarrier,all_digits_generalcargo],axis=0)\n",
    "    all_digits = (all_digits.astype(\"float32\")/ 255.0)*2-1\n",
    "\n",
    "    all_labels = np.concatenate([all_labels_tanker, all_labels_container,all_labels_bulkcarrier,all_labels_generalcargo], axis=0)\n",
    "    all_labels = keras.utils.to_categorical(all_labels, 4)\n",
    "    all_labels = all_labels*2-1\n",
    "    print('input shape: ',all_digits.shape)\n",
    "    print('label shape: ', all_labels.shape)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_digits,all_labels)).shuffle(250).batch(args.batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def recover(image):\n",
    "    image_ = image.numpy()\n",
    "    image__ = np.round(0.5*(image_+1)*255).astype(np.int32)\n",
    "    return image__\n",
    "\n",
    "def plot(fake_sample, epoch):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(fake_sample.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(fake_sample[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('epochpic/epoch_%d.png' % epoch)\n",
    "    return None\n",
    "\n",
    "def train_step_gen(args, one_hot_labels):\n",
    "    batch_size = one_hot_labels.get_shape().as_list()[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        noise = tf.random.uniform([batch_size, args.noise_dim], -1.0, 1.0)\n",
    "        noise = tf.cast(noise, tf.float32)\n",
    "        one_hot_labels = tf.cast(one_hot_labels, tf.float32)\n",
    "        random_labels = tf.concat(\n",
    "            [noise, one_hot_labels], axis=1\n",
    "        )\n",
    "        fake_sample = args.gen(random_labels)\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[256 * 256]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, 256, 256, 4)\n",
    "        )\n",
    "\n",
    "        fake_sample = tf.concat([fake_sample, image_one_hot_labels], -1)\n",
    "\n",
    "        fake_score = args.dis(fake_sample)\n",
    "        loss = - tf.reduce_mean(fake_score)\n",
    "    gradients = tape.gradient(loss, args.gen.trainable_variables)\n",
    "    args.gen_opt.apply_gradients(zip(gradients, args.gen.trainable_variables))\n",
    "    args.gen_loss(loss)\n",
    "\n",
    "def train_step_dis(args, real_sample, one_hot_labels):\n",
    "    batch_size = real_sample.get_shape().as_list()[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        noise = tf.random.uniform([batch_size, args.noise_dim], -1.0, 1.0)\n",
    "        noise = tf.cast(noise, tf.float32)\n",
    "        one_hot_labels = tf.cast(one_hot_labels, tf.float32)\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[256 * 256]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, 256, 256, 4)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        noise_label = tf.concat(\n",
    "            [noise, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        fake_sample = args.gen(noise_label)\n",
    "        fake_sample = tf.concat([fake_sample, image_one_hot_labels], -1)\n",
    "        real_sample = tf.concat([real_sample, image_one_hot_labels], -1)\n",
    "\n",
    "        real_score = args.dis(real_sample)\n",
    "        fake_score = args.dis(fake_sample)\n",
    "\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        inter_sample = fake_sample * alpha + real_sample * (1 - alpha)\n",
    "        with tf.GradientTape() as tape_gp:\n",
    "            tape_gp.watch(inter_sample)\n",
    "            inter_score = args.dis(inter_sample)\n",
    "        gp_gradients = tape_gp.gradient(inter_score, inter_sample)\n",
    "        gp_gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradients), axis = [1, 2, 3]))\n",
    "        gp = tf.reduce_mean((gp_gradients_norm - 1.0) ** 2)\n",
    "\n",
    "        loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score) + gp * args.gp_lambda\n",
    "\n",
    "    gradients = tape.gradient(loss, args.dis.trainable_variables)\n",
    "    args.dis_opt.apply_gradients(zip(gradients, args.dis.trainable_variables))\n",
    "\n",
    "    args.dis_loss(loss)\n",
    "    args.adv_loss(loss - gp * args.gp_lambda)\n",
    "\n",
    "def test_step(args, epoch):\n",
    "    noise = tf.random.uniform([64, args.noise_dim], -1.0, 1.0)\n",
    "    one_hot_labels = np.eye(4)[np.random.choice(4, 64)]\n",
    "    one_hot_labels = one_hot_labels * 2 - 1\n",
    "    random_labels = tf.concat(\n",
    "        [noise, one_hot_labels], axis=1\n",
    "    )\n",
    "    fake_sample = args.gen(random_labels)\n",
    "    fake_sample = recover(fake_sample)\n",
    "    plot(fake_sample, epoch)\n",
    "\n",
    "def train(args):\n",
    "    for epoch in range(1,args.n_epoch+1):\n",
    "        cnt = 0\n",
    "        for batch in args.ds:\n",
    "            real_images, one_hot_labels = batch\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt % (args.n_update_dis + 1) > 0:\n",
    "                train_step_dis(args, real_images, one_hot_labels)\n",
    "            else:\n",
    "                train_step_gen(args, one_hot_labels)\n",
    "\n",
    "        if epoch == 1 or epoch % 50 == 0:\n",
    "            test_step(args, epoch)\n",
    "            args.gen.save_weights(\"generator.weights.h5\")\n",
    "\n",
    "        template = 'Epoch {}, Gen Loss: {}, Dis Loss: {}, Adv Loss: {}'\n",
    "        print (template.format(epoch, args.gen_loss.result(),\n",
    "                args.dis_loss.result(), args.adv_loss.result()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_arguments()\n",
    "    args.ds = Dataset(args)\n",
    "\n",
    "    # Initialize Networks\n",
    "    args.gen = Generator()\n",
    "    args.dis = Discriminator()\n",
    "\n",
    "    # Initialize Optimizer\n",
    "    args.gen_opt = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "    args.dis_opt = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    args.adv_loss = tf.keras.metrics.Mean(name = 'Adversarial_Loss')\n",
    "    args.gen_loss = tf.keras.metrics.Mean(name = 'Generator_Loss')\n",
    "    args.dis_loss = tf.keras.metrics.Mean(name = 'Discriminator_Loss')\n",
    "\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4546803,
     "sourceId": 7772001,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
